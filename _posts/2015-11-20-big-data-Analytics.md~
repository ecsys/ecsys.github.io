---
layout: post
title: Intro to Hadoop and MapReduce - Notes
permalink: /intro-to-hadoop-and-mapreduce.html
category: Tech
tag: big data, hadoop
---

## Some Definitions to Begin with

- Big data - the data that is too big to be processed on a single machine
- Volume - the size of the data
- Variety - data comes from a lot of sources and in different formats
- Velocity - the speed at which data is generated
- Core Hadoop
	- HDFS - Hadoop distributed file system, where the data is stored in a Hadoop project
	- Cluster - A cluster is group of machines that are used to store the data
	- MapReduce - used to process data
- Hadoop Ecosystem - programs that are related to core Hadoop to make it easier to use
	- Hive turns SQL into mapreduce code (good at running long batch processing jobs)
	- Pig allow users use simple script to analyse data rather that map reduce
	- Impala provides a way to query data in HDFS with SQL without converting it to map reduce code (good at low latency queries)
	- Sqoop put SQL database into HDFS
	- HBase is a realtime database built on HDFS
	- only to name a few, there are other tools including visulization tools, etc.

## Hadoop and MapReduce

### Intro to HDFS

When a file is loaded into HDFS, it is splitted to **blocks**(64 MB by default, name as blk_number). Blocks are stored in **datanodes** and the matadata of blocks are stored in a **namenode**. 

Hadoop replicates each block three times which means each block lives in three different datanodes. So if a single datanode fails, we have two other backup copies of the blocks in that node. When the namenode detects a faliure of a datanode, it will replicate all blocks in that node again so that we could have three copies of each block again.

What if the namenode fails? Nowadays, we usually have two namenode in a cluster. One is the actice namenode and the other is the standby namenode in case the active one fails.

HDFS scripting is similar to UNIX shell script.  
`hadoop fs -put filename` to upload a file to HDFS
`hadoop fs -tail filename` to display last few lines of the file
`hadoop fs -rm filename` to delete a file in HDFS

### Intro to MapReduce

MapReduce divides a file into chuncks and then process it in parallel. If we have a long file storing information of all sale records of different stores. Say we want to find the total amount of sales of each store. Traditionally we would have a hashtable where the key is name of the store and the value is the sales; and we read the file line by line to update the hashtable. This may cost super long time. MapReduce divide the original large file into chuncks and assign each chunck to a **mapper**. The mapper will read its chunck and group sale records by store name. Then there will be a few **reducers** collecting information from each mapper. For example, a reducer may collect NYC store's records from each mapper and add up values in these records to get the total sale at NYC store.

**Mapper -> Intermediate Record -> Shuffle and Sort -> Reducer -> Result**

Mappers are small programs deal with small amount of data and work in parallel. The output of mapper is called **intermediate record**, records are in format of (key, value). Once mappers are finished, a phase in mapreduce call **Shuffle and Sort** takes place. The Shuffle is the movement of the intermediate records from mappers to reducers. The Sort is the process that the reducers will sort these records into a sorted order. Finally, each reducer works on one set of records at a time, it gets the key and lists all the values. Then after doing some computation accroding to the program, it outputs the result.

To run mapreduce(run python code using hadoop streaming) on HDFS: `hadoop fs jar hadoopStreamingFilePath.jar -mapper mapper.py -reducer reducer.py -file locationOfMapper.py -file locationOfReducer.py -input inputFileOnHDFS -output outputFileOnHDFS`







